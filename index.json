[
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/10_introduction/10_introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "What you need to know Some of the components that you will be working on are shown in this diagram\nThe CKA exam will test your knowledge of how you interact with these components via API server in order deploy/manage containers - it is hands-on\nIt is all about being fast and accurate\nYou will be tested on (in no particular order)\u0026hellip;..\n Nodes Pods Containers RBAC Services (Networking) Persistent volumes and claims Cluster Management Troubleshooting etc\u0026hellip;.  Lets briefly examine the infrastructure\n  "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/10_introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Photo by Ihor Dvoretskyion UnsplashIntroduction To Kubernetes High level overview of what kubernetes is   The idea of this workshop is to prepare for the CKA exam by doing.\nThere are a few videos intersperse within the workshop, but the intent is not to replace the excellent documenation that is already available. Rather the goal is to test your knowledge using questions to see if you are ready for the exam.\nThe expectation is that you have a Kubernetes cluster ready to be used - minikube, K3s, microk8s, EKS, GKE etc\u0026hellip;\n The following are useful links CKA Overview\nCKA Important Tips\nCKA curriculum\nCandidate Handbook\nKubernetes documentation\nCKA Study and Exam Tips\n"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/20_workload/10_lab1.html",
	"title": "Pods",
	"tags": [],
	"description": "",
	"content": "Challenge Use kubectl to\n Create a pod using image \u0026lsquo;sirfragalot/docker-demo:dcus\u0026rsquo; called lab1 Describe the pod Get pod events Access the webpage running on port 8080 in the container  using the internal IP using port forwarding   Use a filter command to show only this pod   Which node is this pod running on?\n    Expand here to see the solution   Solution  doc reference\n kubectl run lab1 --image=sirfragalot/docker-demo:dcus kubectl describe pod lab1 kubectl get event | grep lab1 kubectl get event --field-selector involvedObject.name=lab1 curl 192.168.43.1:8080 (get the IP from the describe above) kubectl port-forward lab1 8989:8080 (then: curl localhost:8989) kubectl get pod --field-selector=metadata.name=lab1 -o wide     Challenge Use kubectl to\n Create a pod called \u0026lsquo;nginx\u0026rsquo; with the nginx image     Expand here to see the solution   Solution  example 1\n  example 2\n kubectl run nginx --image=nginx     Challenge Use kubectl to create a new pod with the name redis and with the image redis:1.99.\n Identify the problem with the pod.\n  Rectify the problem with the pod and wait until the pod is ready and healthy.\n    Expand here to see the solution   Solution  doc reference\n  ImagePullBackOff\n kubectl run redis --image=redis:1.99 # notice the ImagePullBackoff error kubectl get pod redis # observe the Events section for errors kubectl describe pod redis # in kubernetes 1.23 you can do \u0026#39;get\u0026#39; on events to observe just the namespace events kubectl get events # resolve the issue - there are other ways - edit command or edit yaml,but this is quickest, change the image for the container in the pod # fix redis pod using redis image (lastest image - bad in prodution)  kubectl set image pod/redis redis=redis kubectl get pods     Challenge Use kubectl to\n Create a pod called my-nginx and expose it on container port 8080.     Expand here to see the solution   Solution kubectl run my-nginx --image=nginx --port=8080 kubectl describe pod my-nginx    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/50_cluster/10_lab7.html",
	"title": "TBC",
	"tags": [],
	"description": "",
	"content": "Challenge TBC   Expand here to see the solution   Solution  doc reference\n TBC    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/60_storage/10_lab11.html",
	"title": "TBC",
	"tags": [],
	"description": "",
	"content": "Challenge TBC   Expand here to see the solution   Solution  doc reference\n TBC    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/70_trouble/10_lab11.html",
	"title": "TBC",
	"tags": [],
	"description": "",
	"content": "Challenge TBC   Expand here to see the solution   Solution  doc reference\n TBC    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/30_networking/10_lab3.html",
	"title": "Using Services",
	"tags": [],
	"description": "",
	"content": "Challenge Use kubectl to\n Create a deployment kubectl create deployment lab2 --image=sirfragalot/docker-demo:dcus Create a service to expose your deployment Access the service internally from the cluster Access the service externally from the cluster Describe services     Expand here to see the solution   Solution  doc reference\n kubectl expose deployment lab2 --name=lab2-svc --port=80 --target-port=8080 kubectl run -it --image=busybox bash / # wget lab2-svc kubectl expose deployment lab2 --name=lab2-np --type=NodePort --port=8080 kubectl get svc lab2-np curl ifconfig.me curl http://34.82.35.166:30595 (or use your web browser) kubectl describe svc     Challenge Use kubectl to\n Create a pod kubectl run redis --image=redis Create a service called redis-svc to expose the redis pod within the cluster on port 6379 Describe service     Expand here to see the solution   Solution  doc reference\n kubectl expose deployment redis --name=redis-svc --port=6379 kubectl run -it --image=busybox bash / # wget redis-service kubectl describe svc     Challenge Use kubectl to\n Create a pod called my-nginx and expose it on container port 8080.     Expand here to see the solution   Solution kubectl run my-nginx --image=nginx --port=8080 kubectl describe pod my-nginx     Challenge Use kubectl to\n Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd).  The target port for the service should be 80.       Expand here to see the solution   Solution kubectl run httpd --image=httpd:alpine --namespace default kubectl expose pod httpd --port=80 kubectl describe pod my-nginx    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/20_workload/20_lab2.html",
	"title": "Deployments",
	"tags": [],
	"description": "",
	"content": "Challenge Use kubectl to\n Create a deployment called lab2  image \u0026lsquo;sirfragalot/docker-demo:dcus\u0026rsquo; one pod   Scale up to 5 pods Scale back down to 1 pod Rolling update to use image nginx instead (tracking changes) View the revision history Roll back to the previous image View the deployment image     Expand here to see the solution   Solution  doc reference\n kubectl create deployment lab2 --image=sirfragalot/docker-demo:dcus \\  --dry-run=client -o yaml | tee lab2.yaml kubectl apply -f lab2.yaml kubectl scale deployment lab2 --replicas=5 kubectl scale deployment lab2 --replicas=1 kubectl set image deployment/lab2 docker-demo=nginx:latest --record kubectl rollout history deployment/lab2 kubectl rollout undo deployment/lab2 kubectl get deployments \\  --output=custom-columns=NAME:.metadata.name,IMAGE:.spec.template.spec.containers.*.image     Challenge Use kubectl to\n Create a deployment called deployme using image busybox with 3 replicas and command sleep 3600. Update the deployment called deployme using image ubuntu     Expand here to see the solution   Solution  doc reference - exec\n  debug pod - exec\n # create deployment called deployme declarative kubectl create deployment deployme --image=busybox --replicas=3 --dry-run=client -o yaml -- sleep 3600 \u0026gt; deployme.yaml kubectl apply -f deployme-1.yaml # OR (imperative)  kubectl create deployment deployme --image=busybox --replicas=3 -- sleep 3600 # select one pod to look at using exec kubectl exec deployme-XXXXXXXX-XXXX -- ps # alter the image of deployment deployme kubectl set image deployment deployme busybox=ubuntu # check that pods have been deployed kubectl get pods    It is bad practice not to specify the image tag (as it will default to \u0026lsquo;latest\u0026rsquo;). It is best practice is to specify the image tag\n "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/30_networking/20_lab4.html",
	"title": "NodePort Service",
	"tags": [],
	"description": "",
	"content": "Challenge Use kubectl to create a namespace {yourname} and deploy the below into that namespace:\n Deployment  Image = sirfragalot/docker-demo:dcus (listens on port 8080) Replicas = 3 Service Type = NodePort       Expand here to see the solution   Solution  doc reference\n kubectl create namespace yourname kubectl -n yourname create deployment lab4 --image=sirfragalot/docker-demo:dcus --replicas=3 --port=8080 kubectl -n yourname expose deployment lab4 --type=NodePort kubectl -n yourname get svc curl localhost:31xxx  when you specify the \u0026lsquo;\u0026ndash;port\u0026rsquo; in create deployment this is only metadata - a hint that is used for the service to understand which target port to map to.\n   "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/10_introduction/20_people.html",
	"title": "Our People",
	"tags": [],
	"description": "",
	"content": "DazMac - 20+ years Linux/Unix, CKA/CKAD\n"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/20_workload.html",
	"title": "Workload Objects",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Photo by Ihor Dvoretskyion UnsplashWorkload Objects Workload objects can be a simple POD with 1 or many containers. Alternatively other workload objects can be used to manage the lifetime of the POD - deployments, replicasets (don\u0026rsquo;t use them directly), statefulset, daemonset etc\u0026hellip;\nThis image shows the workload objects - only some are required for CKA\nImage by Max BrennerIn this section we are going to focus on PODs and Deployments (and to a lesser extent Services) in order test those areas for CKA.\nIn the diagram below we see the relationship between the 3 workload objects (deployment, replicaset, 3 pods) and 1 Networking Object (the service) "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/20_workload/30_lab3.html",
	"title": "Daemonsets",
	"tags": [],
	"description": "",
	"content": "Challenge Use kubectl to\n Create a daemonset called dset  image quay.io/fluentd_elasticsearch/fluentd:v2.5.2       Expand here to see the solution   Solution  doc reference\n # create a deployment kubectl create deployment dset --image=quay.io/fluentd_elasticsearch/fluentd:v2.5.2 --dry-run=client -o yaml \u0026gt; dset.yaml # edit the yaml to change it from a deployment to daemonset make the below changes kind: Deployment change to --\u0026gt; kind: DaemonSet remove creationTimestamp: null remove replicas: 1 remove strategy remove resources remove status # so it now looks like this apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: dset name: dset spec: selector: matchLabels: app: dset template: metadata: labels: app: dset spec: containers: - image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 name: fluentd # apply the file kubectl apply -f dset.yaml # check the results - this should show the number of pods = number of nodes and node column shows different nodes kubectl get pods -o wide -l=app=dset NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dset-pmrn5 1/1 Running 0 34m 192.168.106.6 k8s-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; dset-tpltj 1/1 Running 0 34m 192.168.104.68 k8s-worker1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;     Bonus Challenge Add a new node to your cluster\n what happens with the daemonset?\n    Expand here to see the solution   Solution  doc reference\n The daemonset will automatically deploy a pod to the new node, see the doc reference which has the following quote\n A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\n   "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/30_networking.html",
	"title": "Networking Objects",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Photo by Ihor Dvoretskyion UnsplashNetworking Objects Image by Max Brenner"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/20_workload/40_lab4.html",
	"title": "CronJobs and Jobs",
	"tags": [],
	"description": "",
	"content": " CronJobs and Jobs aren\u0026rsquo;t in CKA, but are in CKAD\n Challenge Create a job called dicey using\n image kodekloud/throw-dice  The image generates a random number between 1 and 6 and exits. It is a success only if it is a 6 else it is a failure. Check how many times it takes the job to trigger to get a successful outcome.\n   Expand here to see the solution   Solution  doc reference\n # create the job kubectl create job dicey --image=kodekloud/throw-dice # check - you should see how many attempts before rolling a 6 kubectl get pods kubectl logs \u0026lt;pod name\u0026gt;     Challenge Create a job with the above image to\n generate 8 successful outcomes. attempts no more than 15 attempts 3 parallel executions.     Expand here to see the solution   Solution  completions reference parallelism reference backofflimit reference\n #create a job kubectl create job dicey2 --image=kodekloud/throw-dice --dry-run=client -o yaml \u0026gt; dicey2.yaml # edit it so it looks like this apiVersion: batch/v1 kind: Job metadata: name: dicey2 spec: completions: 8 parallelism: 3 backoffLimit: 15 template: metadata: spec: containers: - image: kodekloud/throw-dice name: dicey2 restartPolicy: Never # execute the file kubectl apply -f dicey2.yaml # check the output kubectl get jobs kubectl get pods -l job-name=dicey2 # pick a failed and a succesful job kubectl logs \u0026lt;pod name\u0026gt;    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/40_rbac.html",
	"title": "RBAC",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Photo by Ihor Dvoretskyion UnsplashRBAC Image by Max Brenner"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/50_cluster.html",
	"title": "Cluster Management",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Photo by Ihor Dvoretskyion UnsplashCluster Management Image by Max Brenner"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/20_workload/50_lab5.html",
	"title": "Querying Resources",
	"tags": ["deployments", "services"],
	"description": "",
	"content": "Challenge Deploy a deployment named cache using the image memcached with 3 replicas\n Expose the deployment on a port on the host (NodePort) that sends to port 11211 of the pod. Output the endpoints in json format to q3.json Tracking changes, scale the deployment to 5 replicas Output the history of this deployment to a file called q3.txt     Expand here to see the solution   Solution  doc reference\n # create the deployment kubectl create deployment cache --image=memcached --replicas=3 # expose the deployment using NodePort type kubectl expose deployment cache --type=NodePort --port=11211 # output the endpoints in json format kubectl get ep cache -o json \u0026gt; q3.json # record switch on scale command is deprecated kubectl scale deployment cache --replicas=5 --record # record rollout history kubectl rollout history deployment cache \u0026gt; q3.txt # BONUS detailed history kubectl rollout history deployment cache --revision=1    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/20_workload/60_lab6.html",
	"title": "Labels",
	"tags": ["deployments", "labels"],
	"description": "",
	"content": "Challenge Create a namespace called accounts all the following steps need to be done in this namespace\nDeploy a deployment named redis-app using the image redis with 2 replicas\n with labels tier=prod and loc=north  Deploy a deployment named redis-db using the image redis:alpine with 2 replicas\n with labels tier=prod and loc=south Expose the redis-db deployment on a port 6379.  Use labels to query the pods\n list pod with label tier=prod list pods with label loc=south  Alter deployment redis-app\n add label region=emea change label tier=prod to tier=dev     Expand here to see the solution   Solution  doc reference\n # create the namespace kubectl create ns accounts # create the deployments (imperative for speed) kubectl create deployment redis-app --image=redis --replicas=2 -n accounts kubectl create deployment redis-db --image=redis:alpine --replicas=2 -n accounts # expose redis-db kubectl expose deployment redis-db --port=6379 -n accounts # change the labels on the deployment kubectl label deployment redis-app tier=prod loc=north -n accounts kubectl label deployment redis-db tier=prod loc=south -n accounts  Are the deployments and there pods labelled the same? (ignore the pod-template-hash ) why?\n Because the label changes were made on the deployments after the pods were created (with the original labels). You would need to label the pods as well to match or restart the deployments to have matching labels\n  # list pods with tier=prod kubectl get pods -n accounts -l tier=prod # list pods with loc=south kubectl get pods -n accounts -l loc=south # Add labels on deployment  kubectl label deployment redis-app region=emea-n accounts # Modify labels. As the label exists we need the overwrite option kubectl label deployment redis-app tier=dev --overwrite -n accounts    "
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/60_storage.html",
	"title": "Storage Objects",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Photo by Ihor Dvoretskyion UnsplashStorage Objects Image by Max Brenner"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/70_trouble.html",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Photo by Ihor Dvoretskyion UnsplashTroubleshooting Image by Max Brenner"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/tags/deployments.html",
	"title": "deployments",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/",
	"title": "Kubernetes Workshop",
	"tags": [],
	"description": "",
	"content": "Kubernetes Workshop Home Welcome Kubernetes Workshop.\nThis site is built to act as a workshop that can be done either as part of conference or bootcamp or spread over a number of smaller sessions.\n"
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/tags/labels.html",
	"title": "labels",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/tags/services.html",
	"title": "services",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://unixdaddy.github.io/kubernetes-workshop-source/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]